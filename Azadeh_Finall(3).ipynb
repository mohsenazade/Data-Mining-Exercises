{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Azadeh_Finall(3).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aBdlk8QFEf7o",
        "hYhQIXAZ869_",
        "ISY5Kv7pE8_S",
        "M4JoPYDoCGZJ",
        "qBMH5H5B8OnI",
        "1OBNh0cu8FPC",
        "bqBnK0Mz9I1t",
        "OM98uUSv9S0V",
        "TgIF4TqB900-",
        "-Gq4tfOo94YP"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohsenazade/Data-Mining-Exercises/blob/main/Azadeh_Finall(3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAsl7oBF59n-"
      },
      "source": [
        "#Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yiXyb9b6bz_",
        "outputId": "a6d03298-8008-457c-89b2-2f7e72aa8395"
      },
      "source": [
        "import pandas as pd\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\r\n",
        "from sklearn.base import TransformerMixin\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "import nltk\r\n",
        "import spacy\r\n",
        "\r\n",
        "from nltk.corpus import sentiwordnet as swn\r\n",
        "import re\r\n",
        "from nltk.corpus import wordnet as wn\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
        "from nltk import word_tokenize, pos_tag\r\n",
        "from collections import defaultdict\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('sentiwordnet')\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn6r2wN069xG"
      },
      "source": [
        "#import other lists of stopwords\r\n",
        "with open('/content/StopWords_GenericLong.txt', 'r') as f:\r\n",
        " x_gl = f.readlines()\r\n",
        "with open('/content/StopWords_Names.txt', 'r') as f:\r\n",
        " x_n = f.readlines()\r\n",
        "with open('/content/StopWords_DatesandNumbers.txt', 'r') as f:\r\n",
        " x_d = f.readlines()\r\n",
        "#import nltk stopwords\r\n",
        "stopwords = nltk.corpus.stopwords.words('english')#combine all stopwords\r\n",
        "[stopwords.append(x.rstrip()) for x in x_gl]\r\n",
        "[stopwords.append(x.rstrip()) for x in x_n]\r\n",
        "[stopwords.append(x.rstrip()) for x in x_d]\r\n",
        "#change all stopwords into lowercase\r\n",
        "stopwords_lower = [s.lower() for s in stopwords]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBdlk8QFEf7o"
      },
      "source": [
        "#DataSet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYhQIXAZ869_"
      },
      "source": [
        "#####***Load Data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "kCLfVqZQ7GrM",
        "outputId": "46905e71-ccd5-4e33-b27a-0cbb21097462"
      },
      "source": [
        "# Loading csv file\r\n",
        "df =pd.read_csv('/content/BTCUSDnews_Azadeh_Label.csv', usecols=['Label','title','articleBody'],encoding = \"ISO-8859-1\")\r\n",
        "twe=df[['Label','articleBody']]\r\n",
        "twe['label']=twe['Label']\r\n",
        "twe['totallText']=twe['articleBody']\r\n",
        "totall=twe[['label','totallText']]\r\n",
        "totall.head(3)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-119c1267c77b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/BTCUSDnews_Azadeh_Label.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'articleBody'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtwe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'articleBody'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtwe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtwe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtwe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'totallText'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtwe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'articleBody'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1991\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1993\u001b[0;31m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1994\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/BTCUSDnews_Azadeh_Label.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km0odMCUEcfW"
      },
      "source": [
        "totall.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISY5Kv7pE8_S"
      },
      "source": [
        "#####***Splitting into train and test***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdnqSlKEFCzm"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "totall_train, totall_test, target_train, target_test = train_test_split(totall.totallText,totall.label,test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85XlvbmwGPVW"
      },
      "source": [
        "totall_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQgJXuZ6GnWu"
      },
      "source": [
        "totall_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgUlsSBoGchH"
      },
      "source": [
        "totall_train.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdc4s8i0Nq60"
      },
      "source": [
        "totall_test.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bVHgggTxv0W"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "fig = plt.figure(figsize=(8,6))\r\n",
        "df.groupby('Label').count().plot.bar(ylim=0)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4JoPYDoCGZJ"
      },
      "source": [
        "#***Text Processing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMH5H5B8OnI"
      },
      "source": [
        "#####***A. Cleaning of Raw Data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2LYMx2J7vMl"
      },
      "source": [
        "def to_lower(word): \r\n",
        "     result = word.lower() \r\n",
        "     return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc98BDV5HEwC"
      },
      "source": [
        "def remove_number(word):\r\n",
        "    result = re.sub(r'\\d+', '', word)\r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PurhnIl7yvx"
      },
      "source": [
        "def remove_mentions(word):       \r\n",
        "    result = re.sub(r\"@\\S+\", \"\", word)       \r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDp4KIi173WN"
      },
      "source": [
        "def remove_special_characters(word):       \r\n",
        "    result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))    \r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCM82GtRHycS"
      },
      "source": [
        "def remove_stopwords(word):\r\n",
        "    return ' '.join(word for word in i.split() if word not in stopwords_lower)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WQv--41HgGf"
      },
      "source": [
        "def remove_whitespace(word):\r\n",
        "    result = word.strip()\r\n",
        "    return result\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8JXfUdEHd2n"
      },
      "source": [
        "def remove_punctuation(word):\r\n",
        "    result = re.sub('[^A-Za-z]+', ' ', word)\r\n",
        "    return result\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2MLfCSSHsmF"
      },
      "source": [
        "def replace_newline(word):\r\n",
        "    return word.replace('\\n','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4131L7l8CTW"
      },
      "source": [
        "def remove_hyperlink(word):       \r\n",
        "    return re.sub(r\"http\\S+\", \"\", word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mYW-AlBH-lz"
      },
      "source": [
        "def clean_up_pipeline(sentence):\r\n",
        "    cleaning_data = [remove_hyperlink,\r\n",
        "                      replace_newline,\r\n",
        "                      to_lower,\r\n",
        "                      remove_number,\r\n",
        "                      remove_punctuation,\r\n",
        "                      remove_whitespace]\r\n",
        "    for func in cleaning_data:\r\n",
        "        \r\n",
        "        sentence = func(sentence)\r\n",
        "    return sentence\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-3qxN4QIEWu"
      },
      "source": [
        "totall_train = totall_train.apply(clean_up_pipeline)\r\n",
        "print(totall_train)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OBNh0cu8FPC"
      },
      "source": [
        "#####***B. Tokenization***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XMIcOMk8W73"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\r\n",
        "totall_train = totall_train.apply(word_tokenize)\r\n",
        "#cleaned_tweets_test = cleaned_tweets_test.apply(word_tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG4IgI99I-KA"
      },
      "source": [
        "totall_train.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqBnK0Mz9I1t"
      },
      "source": [
        "#####***C. Stemming***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TYnKDg89SX9"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\r\n",
        "stemmer = PorterStemmer()\r\n",
        "def stem_words(text):\r\n",
        "    return \" \".join([stemmer.stem(word) for word in text])\r\n",
        "totall_train = totall_train.apply(lambda text: stem_words(text))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NadSTcDZOaTv"
      },
      "source": [
        "totall_train.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM98uUSv9S0V"
      },
      "source": [
        "#***Word Embedding Techniques***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chFCNVwH9bwT"
      },
      "source": [
        "#####***Word2Vec***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IwDPw5o9w6r"
      },
      "source": [
        "import gensim\r\n",
        "tokenize=totall_train.apply(lambda x: x.split())\r\n",
        "w2vec_model=gensim.models.Word2Vec(tokenize,min_count = 1, size = 100, window = 5, sg = 1)\r\n",
        "w2vec_model.train(tokenize,total_examples= len(totall_train),epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg7w3q68Ju3S"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfBagXrOJyhm"
      },
      "source": [
        "bow=CountVectorizer( min_df=2, max_features=1000)\r\n",
        "bow.fit(totall_train)\r\n",
        "bow_df=bow.transform(totall_train).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfZxeYL4J1Aq"
      },
      "source": [
        "bow_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgIF4TqB900-"
      },
      "source": [
        "#####***Model Fitting***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1R1hhkA97qE"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn import linear_model\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "import numpy as np\r\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3chYIqVzJ9GS"
      },
      "source": [
        "target_train =np.array(target_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gjdxfgpS1Hr"
      },
      "source": [
        "from textblob import TextBlob\r\n",
        "\r\n",
        "tresult = [TextBlob(i).sentiment.polarity for i in totall_train]\r\n",
        "tpred = [0 if n < 0 else 1 for n in tresult]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gq4tfOo94YP"
      },
      "source": [
        "#####**Performance Metrics for News Bitcoin Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1JQolU8yPNJ"
      },
      "source": [
        "vec = CountVectorizer() \r\n",
        "\r\n",
        "X_train_transformed =  vec.fit_transform(totall_train) \r\n",
        "\r\n",
        "X_test_transformed = vec.transform(totall_test)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0zJO78aPbkC"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.metrics import confusion_matrix,f1_score, precision_score,recall_score\r\n",
        "\r\n",
        "\r\n",
        "clf= MultinomialNB()\r\n",
        "clf.fit(X_train_transformed, target_train)\r\n",
        "\r\n",
        "score = clf.score(X_test_transformed, target_test)\r\n",
        "print(\"score of Naive Bayes algo is :\\n   \" , score)\r\n",
        "\r\n",
        "y_pred = clf.predict(X_test_transformed)\r\n",
        "cf_matrix=confusion_matrix(target_test,y_pred)\r\n",
        "print(cf_matrix)\r\n",
        "\r\n",
        "print(precision_score(target_test,y_pred,pos_label=1,average='macro'))\r\n",
        "print(recall_score(target_test, y_pred, pos_label=1,average='macro') )\r\n",
        "\r\n",
        "print(\"F1_Score :\",f1_score(target_test, y_pred, pos_label=1,average='weighted'))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4ZBC9MXKZ6y"
      },
      "source": [
        "import seaborn as sns\r\n",
        "sns.heatmap(cf_matrix, annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbiuFs45yerY"
      },
      "source": [
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOax3OfVyb2-"
      },
      "source": [
        "model = LinearSVC()\r\n",
        "#X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.33, random_state=0)\r\n",
        "model.fit(X_train_transformed, target_train)\r\n",
        "y_pred = model.predict(X_test_transformed)\r\n",
        "\r\n",
        "score = model.score(X_test_transformed, target_test)\r\n",
        "print(\"score of SVM is :\\n   \" , score)\r\n",
        "\r\n",
        "y_pred = model.predict(X_test_transformed)\r\n",
        "cf_matrix=confusion_matrix(target_test,y_pred)\r\n",
        "print(cf_matrix)\r\n",
        "\r\n",
        "print(precision_score(target_test,y_pred,pos_label=1,average='macro'))\r\n",
        "print(recall_score(target_test, y_pred, pos_label=1,average='macro') )\r\n",
        "\r\n",
        "print(\"F1_Score :\",f1_score(target_test, y_pred, pos_label=1,average='weighted'))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmFBPxuAynQA"
      },
      "source": [
        "import seaborn as sns\r\n",
        "sns.heatmap(cf_matrix, annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}